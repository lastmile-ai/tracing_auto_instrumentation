{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# _____ ______  _______      _  _  _ _______ _______ _______  _____  __   _      _     _\n",
    "#   |   |_____] |  |  |      |  |  | |_____|    |    |______ |     | | \\  |       \\___/ \n",
    "# __|__ |_____] |  |  |      |__|__| |     |    |    ______| |_____| |  \\_|      _/   \\_                                                                                       \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tracing_auto_instrumentation in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (0.0.1)\n",
      "Requirement already satisfied: lastmile-eval in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from tracing_auto_instrumentation) (0.0.46)\n",
      "Requirement already satisfied: ibm-watsonx-ai in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from tracing_auto_instrumentation) (1.0.4)\n",
      "Requirement already satisfied: requests in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (2.32.2)\n",
      "Requirement already satisfied: urllib3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (2.1.0)\n",
      "Requirement already satisfied: pandas<2.2.0,>=0.24.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (2.1.2)\n",
      "Requirement already satisfied: certifi in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (2024.2.2)\n",
      "Requirement already satisfied: lomond in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (0.3.3)\n",
      "Requirement already satisfied: tabulate in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (0.9.0)\n",
      "Requirement already satisfied: packaging in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (23.2)\n",
      "Requirement already satisfied: ibm-cos-sdk<2.14.0,>=2.12.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (2.13.4)\n",
      "Requirement already satisfied: importlib-metadata in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-watsonx-ai->tracing_auto_instrumentation) (7.0.0)\n",
      "Requirement already satisfied: dataclasses in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.6)\n",
      "Requirement already satisfied: python-dotenv in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.0.0)\n",
      "Requirement already satisfied: rouge-score in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.2)\n",
      "Requirement already satisfied: nltk in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.30.1)\n",
      "Requirement already satisfied: python-aiconfig in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.1.34)\n",
      "Requirement already satisfied: pydantic>=2.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (2.7.1)\n",
      "Requirement already satisfied: anthropic in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.26.1)\n",
      "Requirement already satisfied: evaluate==0.4.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.4.1)\n",
      "Requirement already satisfied: arize-phoenix-evals==0.5.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.5.0)\n",
      "Requirement already satisfied: instructor in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.2.6)\n",
      "Requirement already satisfied: opentelemetry-api in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-sdk in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.45b0)\n",
      "Requirement already satisfied: openinference-semantic-conventions in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.6)\n",
      "Requirement already satisfied: fastapi==0.110.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.110.1)\n",
      "Requirement already satisfied: jinja2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (3.1.4)\n",
      "Requirement already satisfied: uvicorn in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.29.0)\n",
      "Requirement already satisfied: result in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.16.0)\n",
      "Requirement already satisfied: lastmile-utils in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.0.24)\n",
      "Requirement already satisfied: email-validator==2.1.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (2.1.1)\n",
      "Requirement already satisfied: llama-index in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.10.38)\n",
      "Requirement already satisfied: llama-index-embeddings-openai in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.10)\n",
      "Requirement already satisfied: llama-index-readers-web in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.16)\n",
      "Requirement already satisfied: llama-index-callbacks-openinference in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.4)\n",
      "Requirement already satisfied: html2text in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (2020.1.16)\n",
      "Requirement already satisfied: pyarrow in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (16.1.0)\n",
      "Requirement already satisfied: tqdm in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (4.66.4)\n",
      "Requirement already satisfied: openinference-instrumentation-llama-index in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (1.4.1)\n",
      "Requirement already satisfied: openinference-instrumentation-langchain in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.16)\n",
      "Requirement already satisfied: langchain in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.2.0)\n",
      "Requirement already satisfied: langchain-core in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.2.1)\n",
      "Requirement already satisfied: langchain-openai in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-eval->tracing_auto_instrumentation) (0.1.7)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.5 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from arize-phoenix-evals==0.5.0->lastmile-eval->tracing_auto_instrumentation) (4.11.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from email-validator==2.1.1->lastmile-eval->tracing_auto_instrumentation) (2.6.1)\n",
      "Requirement already satisfied: idna>=2.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from email-validator==2.1.1->lastmile-eval->tracing_auto_instrumentation) (3.7)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (2.19.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (1.26.4)\n",
      "Requirement already satisfied: dill in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (2024.3.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (0.21.4)\n",
      "Requirement already satisfied: responses<0.19 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (0.18.0)\n",
      "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from fastapi==0.110.1->lastmile-eval->tracing_auto_instrumentation) (0.37.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai->tracing_auto_instrumentation) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai->tracing_auto_instrumentation) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pandas<2.2.0,>=0.24.2->ibm-watsonx-ai->tracing_auto_instrumentation) (2024.1)\n",
      "Requirement already satisfied: ibm-cos-sdk-core==2.13.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai->tracing_auto_instrumentation) (2.13.4)\n",
      "Requirement already satisfied: ibm-cos-sdk-s3transfer==2.13.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai->tracing_auto_instrumentation) (2.13.4)\n",
      "Requirement already satisfied: jmespath<=1.0.1,>=0.10.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ibm-cos-sdk<2.14.0,>=2.12.0->ibm-watsonx-ai->tracing_auto_instrumentation) (1.0.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openai>=1.0.0->lastmile-eval->tracing_auto_instrumentation) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openai>=1.0.0->lastmile-eval->tracing_auto_instrumentation) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openai>=1.0.0->lastmile-eval->tracing_auto_instrumentation) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openai>=1.0.0->lastmile-eval->tracing_auto_instrumentation) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pydantic>=2.1->lastmile-eval->tracing_auto_instrumentation) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pydantic>=2.1->lastmile-eval->tracing_auto_instrumentation) (2.18.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from requests->ibm-watsonx-ai->tracing_auto_instrumentation) (3.3.2)\n",
      "Requirement already satisfied: jiter<1,>=0.1.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from anthropic->lastmile-eval->tracing_auto_instrumentation) (0.1.0)\n",
      "Requirement already satisfied: tokenizers>=0.13.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from anthropic->lastmile-eval->tracing_auto_instrumentation) (0.19.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from importlib-metadata->ibm-watsonx-ai->tracing_auto_instrumentation) (3.18.2)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from instructor->lastmile-eval->tracing_auto_instrumentation) (3.9.5)\n",
      "Requirement already satisfied: docstring-parser<0.17,>=0.16 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from instructor->lastmile-eval->tracing_auto_instrumentation) (0.16)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.7.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from instructor->lastmile-eval->tracing_auto_instrumentation) (13.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from instructor->lastmile-eval->tracing_auto_instrumentation) (8.3.0)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.9.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from instructor->lastmile-eval->tracing_auto_instrumentation) (0.12.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from jinja2->lastmile-eval->tracing_auto_instrumentation) (2.1.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain->lastmile-eval->tracing_auto_instrumentation) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain->lastmile-eval->tracing_auto_instrumentation) (2.0.30)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain->lastmile-eval->tracing_auto_instrumentation) (0.6.6)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain->lastmile-eval->tracing_auto_instrumentation) (0.2.0)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain->lastmile-eval->tracing_auto_instrumentation) (0.1.60)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain-core->lastmile-eval->tracing_auto_instrumentation) (1.33)\n",
      "Requirement already satisfied: tiktoken<1,>=0.7 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langchain-openai->lastmile-eval->tracing_auto_instrumentation) (0.7.0)\n",
      "Requirement already satisfied: black==23.11.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (23.11.0)\n",
      "Requirement already satisfied: chardet==5.2.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (5.2.0)\n",
      "Requirement already satisfied: flake8==6.1.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (6.1.0)\n",
      "Requirement already satisfied: isort==5.12.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (5.12.0)\n",
      "Requirement already satisfied: pylint==3.0.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (3.0.2)\n",
      "Requirement already satisfied: pyright==1.1.335 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (1.1.335)\n",
      "Requirement already satisfied: pytest==7.4.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (7.4.3)\n",
      "Requirement already satisfied: autoflake==2.2.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (2.2.1)\n",
      "Requirement already satisfied: pyflakes>=3.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from autoflake==2.2.1->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (3.1.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from black==23.11.0->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (8.1.7)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from black==23.11.0->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (1.0.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from black==23.11.0->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (0.12.1)\n",
      "Requirement already satisfied: platformdirs>=2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from black==23.11.0->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (4.2.2)\n",
      "Requirement already satisfied: mccabe<0.8.0,>=0.7.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from flake8==6.1.0->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (0.7.0)\n",
      "Requirement already satisfied: pycodestyle<2.12.0,>=2.11.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from flake8==6.1.0->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (2.11.1)\n",
      "Requirement already satisfied: astroid<=3.1.0-dev0,>=3.0.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pylint==3.0.2->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (3.0.3)\n",
      "Requirement already satisfied: tomlkit>=0.10.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pylint==3.0.2->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (0.12.5)\n",
      "Requirement already satisfied: nodeenv>=1.6.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pyright==1.1.335->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (1.8.0)\n",
      "Requirement already satisfied: iniconfig in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pytest==7.4.3->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pytest==7.4.3->lastmile-utils->lastmile-eval->tracing_auto_instrumentation) (1.5.0)\n",
      "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.2.5)\n",
      "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.12)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.38 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.10.38.post2)\n",
      "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.6)\n",
      "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.9.48)\n",
      "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.20)\n",
      "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.6)\n",
      "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.6)\n",
      "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.3)\n",
      "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.22)\n",
      "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (4.12.3)\n",
      "Requirement already satisfied: chromedriver-autoinstaller<0.7.0,>=0.6.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.6.4)\n",
      "Requirement already satisfied: newspaper3k<0.3.0,>=0.2.8 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.2.8)\n",
      "Requirement already satisfied: playwright<2.0,>=1.30 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (1.44.0)\n",
      "Requirement already satisfied: selenium<5.0.0,>=4.17.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (4.21.0)\n",
      "Requirement already satisfied: spider-client<0.0.12,>=0.0.11 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.0.11)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from lomond->ibm-watsonx-ai->tracing_auto_instrumentation) (1.16.0)\n",
      "Requirement already satisfied: joblib in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from nltk->lastmile-eval->tracing_auto_instrumentation) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from nltk->lastmile-eval->tracing_auto_instrumentation) (2024.5.15)\n",
      "Requirement already satisfied: openinference-instrumentation>=0.1.7 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openinference-instrumentation-langchain->lastmile-eval->tracing_auto_instrumentation) (0.1.7)\n",
      "Requirement already satisfied: opentelemetry-instrumentation in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openinference-instrumentation-langchain->lastmile-eval->tracing_auto_instrumentation) (0.45b0)\n",
      "Requirement already satisfied: wrapt in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from openinference-instrumentation-langchain->lastmile-eval->tracing_auto_instrumentation) (1.16.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-api->lastmile-eval->tracing_auto_instrumentation) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc==1.24.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-http==1.24.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (1.64.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.24.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: opentelemetry-proto==1.24.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (1.24.0)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.19 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-proto==1.24.0->opentelemetry-exporter-otlp-proto-grpc==1.24.0->opentelemetry-exporter-otlp->lastmile-eval->tracing_auto_instrumentation) (4.25.3)\n",
      "Requirement already satisfied: anthropic-bedrock in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.8.0)\n",
      "Requirement already satisfied: flask-cors in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (4.0.1)\n",
      "Requirement already satisfied: flask[async] in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (3.0.3)\n",
      "Requirement already satisfied: frozendict in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (2.4.4)\n",
      "Requirement already satisfied: google-generativeai>=0.3.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.5.4)\n",
      "Requirement already satisfied: hypothesis==6.91.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (6.91.0)\n",
      "Requirement already satisfied: nest-asyncio in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (1.6.0)\n",
      "Requirement already satisfied: prompt-toolkit in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (3.0.43)\n",
      "Requirement already satisfied: pybars3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.9.7)\n",
      "Requirement already satisfied: ruamel.yaml in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.18.6)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from hypothesis==6.91.0->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (23.2.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from hypothesis==6.91.0->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (2.4.0)\n",
      "Requirement already satisfied: absl-py in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from rouge-score->lastmile-eval->tracing_auto_instrumentation) (2.1.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from uvicorn->lastmile-eval->tracing_auto_instrumentation) (0.14.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->lastmile-eval->tracing_auto_instrumentation) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->lastmile-eval->tracing_auto_instrumentation) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->lastmile-eval->tracing_auto_instrumentation) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.9.1->instructor->lastmile-eval->tracing_auto_instrumentation) (1.9.4)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (2.5)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->lastmile-eval->tracing_auto_instrumentation) (3.21.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->lastmile-eval->tracing_auto_instrumentation) (0.9.0)\n",
      "Requirement already satisfied: filelock in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (3.14.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate==0.4.1->lastmile-eval->tracing_auto_instrumentation) (0.6)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.6.4)\n",
      "Requirement already satisfied: google-api-core in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (2.19.0)\n",
      "Requirement already satisfied: google-api-python-client in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (2.129.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.4->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (1.23.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai>=1.0.0->lastmile-eval->tracing_auto_instrumentation) (1.0.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->lastmile-eval->tracing_auto_instrumentation) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->lastmile-eval->tracing_auto_instrumentation) (3.10.3)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index->lastmile-eval->tracing_auto_instrumentation) (1.0.8)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index->lastmile-eval->tracing_auto_instrumentation) (0.1.19)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index->lastmile-eval->tracing_auto_instrumentation) (3.3)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-core<0.11.0,>=0.10.38->llama-index->lastmile-eval->tracing_auto_instrumentation) (10.3.0)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->lastmile-eval->tracing_auto_instrumentation) (4.2.0)\n",
      "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index->lastmile-eval->tracing_auto_instrumentation) (0.0.26)\n",
      "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index->lastmile-eval->tracing_auto_instrumentation) (0.4.3)\n",
      "Requirement already satisfied: cssselect>=0.9.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (1.2.0)\n",
      "Requirement already satisfied: lxml>=3.6.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (5.2.2)\n",
      "Requirement already satisfied: feedparser>=5.2.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (6.0.11)\n",
      "Requirement already satisfied: tldextract>=2.0.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (5.1.2)\n",
      "Requirement already satisfied: feedfinder2>=0.0.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.0.4)\n",
      "Requirement already satisfied: jieba3k>=0.35.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.35.1)\n",
      "Requirement already satisfied: tinysegmenter==0.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.3)\n",
      "Requirement already satisfied: greenlet==3.0.3 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (3.0.3)\n",
      "Requirement already satisfied: pyee==11.1.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from playwright<2.0,>=1.30->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (11.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor->lastmile-eval->tracing_auto_instrumentation) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from rich<14.0.0,>=13.7.0->instructor->lastmile-eval->tracing_auto_instrumentation) (2.18.0)\n",
      "Requirement already satisfied: trio~=0.17 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from selenium<5.0.0,>=4.17.2->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.25.1)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from selenium<5.0.0,>=4.17.2->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (0.11.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from typer<1.0.0,>=0.9.0->instructor->lastmile-eval->tracing_auto_instrumentation) (1.5.4)\n",
      "Requirement already satisfied: boto3>=1.28.57 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from anthropic-bedrock->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (1.34.110)\n",
      "Requirement already satisfied: botocore>=1.31.57 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from anthropic-bedrock->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (1.34.110)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from flask[async]->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from flask[async]->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from flask[async]->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (1.8.2)\n",
      "Requirement already satisfied: asgiref>=3.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from flask[async]->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (3.8.1)\n",
      "Requirement already satisfied: setuptools>=16.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from opentelemetry-instrumentation->openinference-instrumentation-langchain->lastmile-eval->tracing_auto_instrumentation) (70.0.0)\n",
      "Requirement already satisfied: wcwidth in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from prompt-toolkit->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.2.13)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pybars3->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.5.1)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from ruamel.yaml->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.2.8)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from boto3>=1.28.57->anthropic-bedrock->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.10.1)\n",
      "Requirement already satisfied: sgmllib3k in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from feedparser>=5.2.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (1.0.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-auth>=2.15.0->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (4.9)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.7.0->instructor->lastmile-eval->tracing_auto_instrumentation) (0.1.2)\n",
      "Requirement already satisfied: requests-file>=1.4 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from tldextract>=2.0.1->newspaper3k<0.3.0,>=0.2.8->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (2.1.0)\n",
      "Requirement already satisfied: outcome in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from trio~=0.17->selenium<5.0.0,>=4.17.2->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (1.3.0.post0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from trio-websocket~=0.9->selenium<5.0.0,>=4.17.2->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium<5.0.0,>=4.17.2->llama-index-readers-web->lastmile-eval->tracing_auto_instrumentation) (1.7.1)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-api-python-client->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (4.1.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.4->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (1.62.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (3.1.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai>=0.3.1->python-aiconfig->lastmile-eval->tracing_auto_instrumentation) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"tracing_auto_instrumentation[ibm]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b7r6/Projects/tracing_auto_instrumentation/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import dotenv\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "\n",
    "from lastmile_eval.rag.debugger.api import LastMileTracer\n",
    "from lastmile_eval.rag.debugger.tracing.sdk import get_lastmile_tracer\n",
    "\n",
    "from tracing_auto_instrumentation.ibm import wrap_watson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mode(Enum):\n",
    "    GENERATE = \"GENERATE\"\n",
    "    GENERATE_TEXT = \"GENERATE_TEXT\"\n",
    "\n",
    "\n",
    "def init_watson_model() -> Model:\n",
    "    # To display example params enter\n",
    "    GenParams().get_example_values()\n",
    "    generate_params = {GenParams.MAX_NEW_TOKENS: 25}\n",
    "\n",
    "    watson_model = Model(\n",
    "        model_id=ModelTypes.GRANITE_13B_CHAT_V2,\n",
    "        params=generate_params,\n",
    "        credentials=dict(\n",
    "            api_key=os.getenv(\"WATSONX_API_KEY\"),\n",
    "            url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "        ),\n",
    "        space_id=os.getenv(\"WATSONX_SPACE_ID\"),\n",
    "        verify=None,\n",
    "        validate=True,\n",
    "    )\n",
    "\n",
    "    return watson_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate(prompt: str, trace_name: str) -> None:\n",
    "    tracer: LastMileTracer = get_lastmile_tracer(\n",
    "        trace_name, os.getenv(\"LASTMILE_API_TOKEN\")\n",
    "    )\n",
    "\n",
    "    watson_model: Model = init_watson_model()\n",
    "\n",
    "    tracer.log(\"start lastmile wrap...\")\n",
    "    wrapper = wrap_watson(watson_model, tracer)\n",
    "    tracer.log(\"lastmile wrap complete.\")\n",
    "\n",
    "\n",
    "    with tracer.start_as_current_span(trace_name) as span:\n",
    "        tracer.log(\"start watsonx generate...\")\n",
    "        response = wrapper.generate(prompt)\n",
    "        tracer.log(f\"watsonx generate: {response=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generate_text(prompt: str, trace_name: str) -> None:\n",
    "    tracer: LastMileTracer = get_lastmile_tracer(\n",
    "        trace_name, os.getenv(\"LASTMILE_API_TOKEN\")\n",
    "    )\n",
    "\n",
    "    watson_model: Model = init_watson_model()\n",
    "\n",
    "    tracer.log(\"start lastmile wrap...\")\n",
    "    wrapper = wrap_watson(watson_model, tracer)\n",
    "    tracer.log(\"lastmile wrap complete.\")\n",
    "\n",
    "    with tracer.start_as_current_span(trace_name) as span:\n",
    "        tracer.log(\"start watsonx generate_text...\")\n",
    "        response = wrapper.generate_text(prompt)\n",
    "        tracer.log(f\"watsonx generate_text: {response=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_generation(mode: Mode, prompt: str, trace_name: str) -> None:\n",
    "    logger.info(\"IBM WatsonX Generation Script Starting...\")\n",
    "\n",
    "    # n.b. required for multiple api keys, make sure `.env` is in your local path\n",
    "    dotenv.load_dotenv()\n",
    "\n",
    "    if Mode.GENERATE == mode:\n",
    "        logger.info(f\"running with mode: {mode}\")\n",
    "        run_generate(prompt, trace_name)\n",
    "    elif Mode.GENERATE_TEXT == mode:\n",
    "        logger.info(f\"running with mode: {mode}\")\n",
    "        run_generate_text(prompt, trace_name)\n",
    "    else:\n",
    "        logger.error(f\"unsupported mode: {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-22 12:41:39,459 - IBM WatsonX Generation Script Starting...\n",
      "2024-05-22 12:41:39,469 - running with mode: Mode.GENERATE\n",
      "2024-05-22 12:41:39,531 - Overriding of current TracerProvider is not allowed\n",
      "2024-05-22 12:41:40,356 - Client successfully initialized\n",
      "2024-05-22 12:41:41,157 - Successfully finished Get available foundation models for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200'\n",
      "2024-05-22 12:41:41,157 - Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200): {\"total_count\":25,\"limit\":200,\"first\":{\"href\":\"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200\"},\"resources\":[{\"model_id\":\"baai/bge-large-en-v1\",\"label\":\"bge-large-en-v1\",\"provider\":\"baai\",\"source\":\"baai\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with version 1.5. It has 335 million parameters and an embedding dimension of 1024.\",\"long_description\":\"This model has multi-functionality like dense retrieval, sparse retrieval, multi-vector, Multi-Linguality, and Multi-Granularity(8192 tokens)\",\"tier\":\"class_c1\",\"number_params\":\"335m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"bigscience/mt0-xxl\",\"label\":\"mt0-xxl-13b\",\"provider\":\"BigScience\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"An instruction-tuned iteration on mT5.\",\"long_description\":\"mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).\",\"tier\":\"class_2\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"codellama/codellama-34b-instruct-hf\",\"label\":\"codellama-34b-instruct-hf\",\"provider\":\"Code Llama\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.\",\"long_description\":\"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.\",\"tier\":\"class_2\",\"number_params\":\"34b\",\"min_shot_size\":0,\"task_ids\":[\"code\"],\"tasks\":[{\"id\":\"code\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}]},{\"model_id\":\"google/flan-t5-xl\",\"label\":\"flan-t5-xl-3b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.\",\"long_description\":\"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-07\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":256},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":128},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.3,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"google/flan-t5-xxl\",\"label\":\"flan-t5-xxl-11b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.\",\"long_description\":\"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"tier\":\"class_2\",\"number_params\":\"11b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"google/flan-ul2\",\"label\":\"flan-ul2-20b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.\",\"long_description\":\"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.\",\"tier\":\"class_3\",\"number_params\":\"20b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"ibm-mistralai/merlinite-7b\",\"label\":\"merlinite-7b\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"label\":\"mixtral-8x7b-instruct-v01-q\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Mixtral-8-7b-instruct-v01-gptq model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16)\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-02-15\"},{\"id\":\"constricted\",\"label\":\"deprecated and constricted\",\"start_date\":\"2024-04-19\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]},{\"id\":\"withdrawn\",\"start_date\":\"2024-06-20\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]}]},{\"model_id\":\"ibm/granite-13b-chat-v2\",\"label\":\"granite-13b-chat-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"versions\":[{\"version\":\"2.1.0\",\"available_date\":\"2024-02-15\"},{\"version\":\"2.0.0\",\"available_date\":\"2023-12-01\"}]},{\"model_id\":\"ibm/granite-13b-instruct-v2\",\"label\":\"granite-13b-instruct-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Please write a summary highlighting the main points of the following text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":40,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Please write a summary highlighting the main points of the following text: {{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":1,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Classify the text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":32,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0006,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"ibm/granite-20b-code-instruct\",\"label\":\"granite-20b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-20b-multilingual\",\"label\":\"granite-20b-multilingual\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}],\"versions\":[{\"version\":\"1.1.0\",\"available_date\":\"2024-04-18\"},{\"version\":\"1.0.0\",\"available_date\":\"2024-03-14\"}]},{\"model_id\":\"ibm/granite-34b-code-instruct\",\"label\":\"granite-34b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"34b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-3b-code-instruct\",\"label\":\"granite-3b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/granite-7b-lab\",\"label\":\"granite-7b-lab\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/granite-8b-code-instruct\",\"label\":\"granite-8b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/slate-125m-english-rtrvr\",\"label\":\"slate-125m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 125 million parameters and an embedding dimension of 768.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"tier\":\"class_c1\",\"number_params\":\"125m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/slate-30m-english-rtrvr\",\"label\":\"slate-30m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 30 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"tier\":\"class_c1\",\"number_params\":\"30m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"intfloat/multilingual-e5-large\",\"label\":\"multilingual-e5-large\",\"provider\":\"intfloat\",\"source\":\"intfloat\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 560 million parameters, has 24 layers and the embedding size is 1024.\",\"long_description\":\"This model gets continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta.\",\"tier\":\"class_c1\",\"number_params\":\"560m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"meta-llama/llama-2-13b-chat\",\"label\":\"llama-2-13b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":2048},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-11-09\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"meta-llama/llama-2-70b-chat\",\"label\":\"llama-2-70b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":900},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-09-07\"}]},{\"model_id\":\"meta-llama/llama-3-70b-instruct\",\"label\":\"llama-3-70b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"meta-llama/llama-3-8b-instruct\",\"label\":\"llama-3-8b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"mistralai/mixtral-8x7b-instruct-v01\",\"label\":\"mixtral-8x7b-instruct-v01\",\"provider\":\"Mistral AI\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":16384},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-17\"}]},{\"model_id\":\"sentence-transformers/all-minilm-l12-v2\",\"label\":\"all-minilm-l12-v2\",\"provider\":\"sentence-transformers\",\"source\":\"sentence-transformers\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with 128 token limit. It has 33.4 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows sentence transformers approach, it maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\",\"tier\":\"class_c1\",\"number_params\":\"33.4m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]}]}\n",
      "2024-05-22 12:41:41,491 - Successfully finished Get next details for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200'\n",
      "2024-05-22 12:41:41,492 - Response(GET https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200): {\"total_count\":25,\"limit\":200,\"first\":{\"href\":\"https://us-south.ml.cloud.ibm.com/ml/v1/foundation_model_specs?version=2024-05-10&space_id=c6dd344f-1aab-4b5e-8edb-2a21fa1e0de0&limit=200\"},\"resources\":[{\"model_id\":\"baai/bge-large-en-v1\",\"label\":\"bge-large-en-v1\",\"provider\":\"baai\",\"source\":\"baai\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with version 1.5. It has 335 million parameters and an embedding dimension of 1024.\",\"long_description\":\"This model has multi-functionality like dense retrieval, sparse retrieval, multi-vector, Multi-Linguality, and Multi-Granularity(8192 tokens)\",\"tier\":\"class_c1\",\"number_params\":\"335m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"bigscience/mt0-xxl\",\"label\":\"mt0-xxl-13b\",\"provider\":\"BigScience\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"An instruction-tuned iteration on mT5.\",\"long_description\":\"mt0-xxl (13B) is an instruction-tuned iteration on mT5. Like BLOOMZ, it was fine-tuned on a cross-lingual task mixture dataset (xP3) using multitask prompted finetuning (MTF).\",\"tier\":\"class_2\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"codellama/codellama-34b-instruct-hf\",\"label\":\"codellama-34b-instruct-hf\",\"provider\":\"Code Llama\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Code Llama is an AI model built on top of Llama 2, fine-tuned for generating and discussing code.\",\"long_description\":\"Code Llama is a pretrained and fine-tuned generative text models with 34 billion parameters. This model is designed for general code synthesis and understanding.\",\"tier\":\"class_2\",\"number_params\":\"34b\",\"min_shot_size\":0,\"task_ids\":[\"code\"],\"tasks\":[{\"id\":\"code\"}],\"model_limits\":{\"max_sequence_length\":16384},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}]},{\"model_id\":\"google/flan-t5-xl\",\"label\":\"flan-t5-xl-3b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"A pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format.\",\"long_description\":\"flan-t5-xl (3B) is a 3 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-07\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":256},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":128},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.3,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"google/flan-t5-xxl\",\"label\":\"flan-t5-xxl-11b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-t5-xxl is an 11 billion parameter model based on the Flan-T5 family.\",\"long_description\":\"flan-t5-xxl (11B) is an 11 billion parameter model based on the Flan-T5 family. It is a pretrained T5 - an encoder-decoder model pre-trained on a mixture of supervised / unsupervised tasks converted into a text-to-text format, and fine-tuned on the Fine-tuned Language Net (FLAN) with instructions for better zero-shot and few-shot performance.\",\"tier\":\"class_2\",\"number_params\":\"11b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"google/flan-ul2\",\"label\":\"flan-ul2-20b\",\"provider\":\"Google\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"flan-ul2 is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net.\",\"long_description\":\"flan-ul2 (20B) is an encoder decoder model based on the T5 architecture and instruction-tuned using the Fine-tuned Language Net (FLAN). Compared to the original UL2 model, flan-ul2 (20B) is more usable for few-shot in-context learning because it was trained with a three times larger receptive field. flan-ul2 (20B) outperforms flan-t5 (11B) by an overall relative improvement of +3.2%.\",\"tier\":\"class_3\",\"number_params\":\"20b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":700},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-07-07\"}]},{\"model_id\":\"ibm-mistralai/merlinite-7b\",\"label\":\"merlinite-7b\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Merlinite-7b is a Mistral-7b-derivative model trained with the LAB methodology, using Mixtral-8x7b-Instruct as a teacher model.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8192},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8192}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\"label\":\"mixtral-8x7b-instruct-v01-q\",\"provider\":\"Mistral AI\",\"tuned_by\":\"IBM\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Mixtral-8-7b-instruct-v01-gptq model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16)\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-02-15\"},{\"id\":\"constricted\",\"label\":\"deprecated and constricted\",\"start_date\":\"2024-04-19\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]},{\"id\":\"withdrawn\",\"start_date\":\"2024-06-20\",\"alternative_model_ids\":[\"ibm-mistralai/mixtral-8x7b-instruct-v01\"]}]},{\"model_id\":\"ibm/granite-13b-chat-v2\",\"label\":\"granite-13b-chat-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"versions\":[{\"version\":\"2.1.0\",\"available_date\":\"2024-02-15\"},{\"version\":\"2.0.0\",\"available_date\":\"2023-12-01\"}]},{\"model_id\":\"ibm/granite-13b-instruct-v2\",\"label\":\"granite-13b-instruct-v2\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":0,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":2},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Please write a summary highlighting the main points of the following text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":40,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Please write a summary highlighting the main points of the following text: {{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":1,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":2}},{\"id\":\"classification\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"Classify the text:\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"Input: {{input}} Output:\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":32,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0006,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"text\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"id\":\"extraction\",\"ratings\":{\"quality\":2}}],\"model_limits\":{\"max_sequence_length\":8192,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-12-01\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":16,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.0002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"ibm/granite-20b-code-instruct\",\"label\":\"granite-20b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-20b-multilingual\",\"label\":\"granite-20b-multilingual\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"20b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":3}},{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-03-14\"}],\"versions\":[{\"version\":\"1.1.0\",\"available_date\":\"2024-04-18\"},{\"version\":\"1.0.0\",\"available_date\":\"2024-03-14\"}]},{\"model_id\":\"ibm/granite-34b-code-instruct\",\"label\":\"granite-34b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"34b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":8191},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":8191}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-06\"}]},{\"model_id\":\"ibm/granite-3b-code-instruct\",\"label\":\"granite-3b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"3b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/granite-7b-lab\",\"label\":\"granite-7b-lab\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"7b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"retrieval_augmented_generation\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4095},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/granite-8b-code-instruct\",\"label\":\"granite-8b-code-instruct\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Granite model series is a family of IBM-trained, dense decoder-only models, which are particularly well-suited for generative tasks.\",\"long_description\":\"Granite models are designed to be used for a wide range of generative and non-generative tasks with appropriate prompt engineering. They employ a GPT-style decoder-only architecture, with additional innovations from IBM Research and the open community.\",\"tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"classification\",\"generation\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\"},{\"id\":\"summarization\"},{\"id\":\"classification\"},{\"id\":\"generation\"},{\"id\":\"extraction\"}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-09\"}]},{\"model_id\":\"ibm/slate-125m-english-rtrvr\",\"label\":\"slate-125m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 125 million parameters and an embedding dimension of 768.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"tier\":\"class_c1\",\"number_params\":\"125m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"ibm/slate-30m-english-rtrvr\",\"label\":\"slate-30m-english-rtrvr\",\"provider\":\"IBM\",\"source\":\"IBM\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 30 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows the standard 'sentence transformers' approach, relying on bi-encoders. It generates embeddings for various inputs such as queries, passages, or documents. The training objective is to maximize cosine similarity between two text pieces: text A (query text) and text B (passage text). This process yields sentence embeddings q and p, allowing for comparison through cosine similarity.\",\"tier\":\"class_c1\",\"number_params\":\"30m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"intfloat/multilingual-e5-large\",\"label\":\"multilingual-e5-large\",\"provider\":\"intfloat\",\"source\":\"intfloat\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model. It has 560 million parameters, has 24 layers and the embedding size is 1024.\",\"long_description\":\"This model gets continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta.\",\"tier\":\"class_c1\",\"number_params\":\"560m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]},{\"model_id\":\"meta-llama/llama-2-13b-chat\",\"label\":\"llama-2-13b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"prompt_tune_inferable\"},{\"id\":\"prompt_tune_trainable\"},{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-13b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-13b-chat is a pretrained and fine-tuned generative text model with 13 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_1\",\"number_params\":\"13b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4},\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"generation\",\"tags\":[\"function_prompt_tune_trainable\"]},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096,\"training_data_max_records\":10000},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":2048},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-11-09\"}],\"training_parameters\":{\"init_method\":{\"supported\":[\"random\",\"text\"],\"default\":\"random\"},\"init_text\":{\"default\":\"text\"},\"num_virtual_tokens\":{\"supported\":[20,50,100],\"default\":100},\"num_epochs\":{\"default\":20,\"min\":1,\"max\":50},\"verbalizer\":{\"default\":\"{{input}}\"},\"batch_size\":{\"default\":8,\"min\":1,\"max\":16},\"max_input_tokens\":{\"default\":256,\"min\":1,\"max\":1024},\"max_output_tokens\":{\"default\":128,\"min\":1,\"max\":512},\"torch_dtype\":{\"default\":\"bfloat16\"},\"accumulate_steps\":{\"default\":16,\"min\":1,\"max\":128},\"learning_rate\":{\"default\":0.002,\"min\":0.00001,\"max\":0.5}}},{\"model_id\":\"meta-llama/llama-2-70b-chat\",\"label\":\"llama-2-70b-chat\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-2-70b-chat is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":4096},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":900},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4095}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2023-09-07\"}]},{\"model_id\":\"meta-llama/llama-3-70b-instruct\",\"label\":\"llama-3-70b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-70b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-70b-instruct is a pretrained and fine-tuned generative text model with 70 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_2\",\"number_params\":\"70b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"meta-llama/llama-3-8b-instruct\",\"label\":\"llama-3-8b-instruct\",\"provider\":\"Meta\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"Llama-3-8b-instruct is an auto-regressive language model that uses an optimized transformer architecture.\",\"long_description\":\"Llama-3-8b-instruct is a pretrained and fine-tuned generative text model with 8 billion parameters, optimized for dialogue use cases.\",\"tier\":\"class_1\",\"number_params\":\"8b\",\"min_shot_size\":1,\"task_ids\":[\"question_answering\",\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"question_answering\",\"ratings\":{\"quality\":4}},{\"id\":\"summarization\",\"ratings\":{\"quality\":3}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":4}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":8192},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":4096},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":4096}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-18\"}]},{\"model_id\":\"mistralai/mixtral-8x7b-instruct-v01\",\"label\":\"mixtral-8x7b-instruct-v01\",\"provider\":\"Mistral AI\",\"source\":\"Hugging Face\",\"functions\":[{\"id\":\"text_generation\"}],\"short_description\":\"The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts.\",\"long_description\":\"This model is made with AutoGPTQ, which mainly leverages the quantization technique to 'compress' the model weights from FP16 to 4-bit INT and performs 'decompression' on-the-fly before computation (in FP16). As a result, the GPU memory, and the data transferring between GPU memory and GPU compute engine, compared to the original FP16 model, is greatly reduced. The major quantization parameters used in the process are listed below.\",\"tier\":\"class_1\",\"number_params\":\"46.7b\",\"min_shot_size\":1,\"task_ids\":[\"summarization\",\"retrieval_augmented_generation\",\"classification\",\"generation\",\"code\",\"extraction\"],\"tasks\":[{\"id\":\"summarization\",\"ratings\":{\"quality\":4}},{\"id\":\"retrieval_augmented_generation\",\"ratings\":{\"quality\":3}},{\"id\":\"classification\",\"ratings\":{\"quality\":4}},{\"id\":\"generation\"},{\"id\":\"code\"},{\"id\":\"extraction\",\"ratings\":{\"quality\":4}}],\"model_limits\":{\"max_sequence_length\":32768},\"limits\":{\"lite\":{\"call_time\":\"5m0s\",\"max_output_tokens\":16384},\"v2-professional\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384},\"v2-standard\":{\"call_time\":\"10m0s\",\"max_output_tokens\":16384}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-04-17\"}]},{\"model_id\":\"sentence-transformers/all-minilm-l12-v2\",\"label\":\"all-minilm-l12-v2\",\"provider\":\"sentence-transformers\",\"source\":\"sentence-transformers\",\"functions\":[{\"id\":\"embedding\"}],\"short_description\":\"An embedding model with 128 token limit. It has 33.4 million parameters and an embedding dimension of 384.\",\"long_description\":\"This model follows sentence transformers approach, it maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\",\"tier\":\"class_c1\",\"number_params\":\"33.4m\",\"limits\":{\"lite\":{\"call_time\":\"5m0s\"},\"v2-professional\":{\"call_time\":\"10m0s\"},\"v2-standard\":{\"call_time\":\"10m0s\"}},\"lifecycle\":[{\"id\":\"available\",\"start_date\":\"2024-05-16\"}]}]}\n",
      "2024-05-22 12:41:41,495 - 'start lastmile wrap...'\n",
      "2024-05-22 12:41:41,496 - 'lastmile wrap complete.'\n",
      "2024-05-22 12:41:41,498 - 'start watsonx generate...'\n",
      "2024-05-22 12:41:42,434 - Successfully finished generate for url: 'https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-05-10'\n",
      "2024-05-22 12:41:42,434 - Response(POST https://us-south.ml.cloud.ibm.com/ml/v1/text/generation?version=2024-05-10): {\"model_id\":\"ibm/granite-13b-chat-v2\",\"model_version\":\"2.1.0\",\"created_at\":\"2024-05-22T16:41:42.415Z\",\"results\":[{\"generated_text\":\"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\",\"generated_token_count\":25,\"input_token_count\":13,\"stop_reason\":\"max_tokens\"}]}\n",
      "2024-05-22 12:41:42,533 - invoking tracer to mark query event: prompt='Can you please tell me an amusing anecdote about Thomas Edison?'\n",
      "2024-05-22 12:41:42,538 - did call `mark_rag_query_trace_event`: tracer_res=RAGTraceEventResult(is_success=True, message=\"Logged RAGQueryEvent fully_resolved_prompt='Can you please tell me an amusing anecdote about Thomas Edison?' to span id 'de06532ddf69ad81'\")\n",
      "2024-05-22 12:41:42,538 - invoking tracer to mark query event: llm_output=\"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\"\n",
      "2024-05-22 12:41:42,539 - did call `mark_rag_query_trace_event`: tracer_res=RAGTraceEventResult(is_success=True, message='Logged RAGQueryEvent llm_output=\"\\\\n\\\\nCertainly! One of Edison\\'s most famous inventions was the phonograph, the world\\'s first practical device for recording and\" to span id \\'de06532ddf69ad81\\'')\n",
      "2024-05-22 12:41:42,539 - about to call `register_params`: trace_params={'model_id': 'ibm/granite-13b-chat-v2', 'model_version': '2.1.0', 'llm_output': \"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\", 'generated_tokens': \"\\n\\nCertainly! One of Edison's most famous inventions was the phonograph, the world's first practical device for recording and\", 'generated_token_count': 25, 'input_token_count': 13}\n",
      "2024-05-22 12:41:42,539 - did call `register_params`\n",
      "2024-05-22 12:41:42,540 - 'watsonx generate: response={\\'model_id\\': \\'ibm/granite-13b-chat-v2\\', \\'model_version\\': \\'2.1.0\\', \\'created_at\\': \\'2024-05-22T16:41:42.415Z\\', \\'results\\': [{\\'generated_text\\': \"\\\\n\\\\nCertainly! One of Edison\\'s most famous inventions was the phonograph, the world\\'s first practical device for recording and\", \\'generated_token_count\\': 25, \\'input_token_count\\': 13, \\'stop_reason\\': \\'max_tokens\\'}]}'\n"
     ]
    }
   ],
   "source": [
    "run_generation(\n",
    "    mode=Mode.GENERATE,\n",
    "    prompt=\"Can you please tell me an amusing anecdote about Thomas Edison?\",\n",
    "    trace_name=\"elementary-my-dear-watson\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
